{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPJEKPnEsYB5UGNO7SysVmG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"RFmK2-3nbQ7r","colab_type":"code","outputId":"46ab0966-42bf-4f3f-981c-c2e45ca9c11b","executionInfo":{"status":"ok","timestamp":1588407323044,"user_tz":240,"elapsed":1375,"user":{"displayName":"Jie Lyu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQrhIVVXzQU_KkxCjkumcX5SB8g_Uk4VndpwJ4=s64","userId":"10977586217171880674"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["!nvidia-smi"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Sat May  2 08:15:23 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   61C    P0    45W / 250W |      0MiB / 16280MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lrygyvc2SXoT","colab_type":"code","cellView":"form","outputId":"1ccb40b8-dc1e-4d76-d73a-bbb939bd0e78","executionInfo":{"status":"ok","timestamp":1588407325134,"user_tz":240,"elapsed":3450,"user":{"displayName":"Jie Lyu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQrhIVVXzQU_KkxCjkumcX5SB8g_Uk4VndpwJ4=s64","userId":"10977586217171880674"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["#@title Import\n","# first run you need to do the following\n","# upload {train, test, sample_submission}.csv to ./\n","# !pip install transformers\n","# nltk.download('stopwords')\n","\n","import torch\n","from torch.utils.data import TensorDataset, random_split\n","from torch.utils.data import DataLoader, RandomSampler,SequentialSampler\n","\n","import tensorflow as tf\n","from keras.preprocessing.sequence import pad_sequences\n","\n","from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","\n","import pandas as pd\n","import numpy as np\n","import nltk\n","import re\n","import time\n","import datetime\n","import random\n","import matplotlib.pyplot as plt\n","% matplotlib inline\n","import seaborn as sns\n","\n","# https://mccormickml.com/2019/07/22/BERT-fine-tuning/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"rVH1XoxLXsky","colab_type":"code","outputId":"8984d2f5-7a5b-4098-f50e-55d551ec55a8","executionInfo":{"status":"ok","timestamp":1588407328499,"user_tz":240,"elapsed":6807,"user":{"displayName":"Jie Lyu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQrhIVVXzQU_KkxCjkumcX5SB8g_Uk4VndpwJ4=s64","userId":"10977586217171880674"}},"cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["#@title GPU\n","\n","device_name = tf.test.gpu_device_name()\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')\n","\n","if torch.cuda.is_available():    \n","\n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n","There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"joEaUKaS5U2n","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Helper Functions\n","\n","def clean_text(text_arr):\n","    '''filter out stopwords or words with non-alphabetical characters\n","    \n","    Args:\n","        text_arr: list of strings, each representing sentences\n","    \n","    Returns:\n","        2d list of strings, each representing words\n","    '''\n","    stop_words = set(nltk.corpus.stopwords.words('english'))\n","    tokenizer = nltk.tokenize.TweetTokenizer()\n","    clean = []\n","    \n","    for sent in text_arr:\n","        sent = re.sub(r\"[#@_]\", \" \", sent)\n","        tokens = tokenizer.tokenize(sent)\n","        list_sent = []\n","        for word in tokens:\n","            word = word.strip('#').lower()\n","            if word.isalpha() and not word in stop_words:\n","                list_sent.append(word)\n","        clean.append(list_sent)\n","    return clean\n","\n","def flat_accuracy(preds, labels):\n","    '''calculate the accuracy of our predictions vs labels\n","    '''\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","def tfpn(logit, label, threshold=0.5, beta=2):\n","    '''takes two torch tensors and return 4 torch tenors\n","    '''\n","    softmax = torch.nn.Softmax(dim=1)\n","    sm = softmax(logit)\n","    pred = torch.argmax(sm, dim=1).cpu()\n","    label = (label > threshold).cpu()\n","\n","    TP = (pred & label).sum()\n","    TN = ((~pred) & (~label)).sum()\n","    FP = (pred & (~label)).sum()\n","    FN = ((~pred) & label).sum()\n","\n","    return TP, TN, FP, FN\n","\n","def f1(tfpn_dict):\n","    '''takes a dict of torch tensors and returns a torch tensor\n","    '''\n","    TP, TN, FP, FN = tfpn_dict['TP'], tfpn_dict['TN'], tfpn_dict['FP'], tfpn_dict['FN']\n","    precision = torch.true_divide(TP, TP + FP + 1e-12)\n","    recall = torch.true_divide(TP, TP + FN + 1e-12)\n","    f1 = 2 * (precision * recall) / (precision + recall + 1e-12)\n","    return f1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zJMw2tWZ91Yx","colab_type":"code","colab":{}},"source":["## Parameters\n","\n","maxlen = 15\n","train_val_split = 0.25\n","batch_size = 32\n","random_seed = 84"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VoFqCCjV3qAT","colab_type":"code","cellView":"form","outputId":"3e1aef38-3a40-4b3c-ad55-8b0c087d04d0","executionInfo":{"status":"ok","timestamp":1588412283629,"user_tz":240,"elapsed":1528,"user":{"displayName":"Jie Lyu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQrhIVVXzQU_KkxCjkumcX5SB8g_Uk4VndpwJ4=s64","userId":"10977586217171880674"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["#@title Load Data\n","\n","# tokenize\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","df_train = pd.read_csv('train.csv')\n","x_train_str = clean_text([each for each in df_train['text']])\n","y_train = [each for each in df_train['target']]\n","\n","x_train_ids = []\n","x_train_att_mask = []\n","\n","for sent in x_train_str:\n","    sent = [' '] if len(sent) == 0 else sent\n","    encoded_dict = tokenizer.encode_plus(sent, \n","                                         max_length=maxlen, \n","                                         pad_to_max_length=True, \n","                                         return_attention_mask=True, \n","                                         return_tensors='pt')\n","    x_train_ids.append(encoded_dict['input_ids'])\n","    x_train_att_mask.append(encoded_dict['attention_mask'])\n","\n","x_train_ids = torch.cat(x_train_ids, dim=0)\n","x_train_att_mask = torch.cat(x_train_att_mask, dim=0)\n","y_train = torch.tensor(y_train)\n","\n","# split training for validation\n","\n","dataset = TensorDataset(x_train_ids, x_train_att_mask, y_train)\n","\n","val_size = int((train_val_split) * len(dataset))\n","train_size = len(dataset) - val_size\n","\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print(train_size, 'training samples')\n","print(val_size, 'validation samples')\n","\n","# create dataloaders\n","\n","train_dataloader = DataLoader(train_dataset,\n","                              sampler = RandomSampler(train_dataset),\n","                              batch_size = batch_size)\n","\n","# for validation the order doesn't matter, so just read them in order\n","val_dataloader = DataLoader(val_dataset,\n","                              sampler = SequentialSampler(val_dataset),\n","                              batch_size = batch_size)\n","\n","# testing data\n","\n","df_test = pd.read_csv('test.csv')\n","x_test_str = clean_text([each for each in df_test['text']])\n","\n","x_test_ids = []\n","x_test_att_mask = []\n","\n","for sent in x_test_str:\n","    sent = [' '] if len(sent) == 0 else sent\n","    encoded_dict = tokenizer.encode_plus(sent, \n","                                         max_length=maxlen, \n","                                         pad_to_max_length=True, \n","                                         return_attention_mask=True, \n","                                         return_tensors='pt')\n","    x_test_ids.append(encoded_dict['input_ids'])\n","    x_test_att_mask.append(encoded_dict['attention_mask'])\n","\n","x_test_ids = torch.cat(x_test_ids, dim=0)\n","x_test_att_mask = torch.cat(x_test_att_mask, dim=0)\n","\n","prediction_dataset = TensorDataset(x_test_ids, x_test_att_mask)\n","prediction_dataloader = DataLoader(prediction_dataset,\n","                                   sampler=SequentialSampler(prediction_dataset),\n","                                   batch_size=batch_size)\n","\n","print(len(x_test_str), 'testing inputs')\n","\n","print('\\nbatch size set to', batch_size)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["5710 training samples\n","1903 validation samples\n","3263 testing inputs\n","\n","batch size set to 32\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gzf1kLx7oGdh","colab_type":"code","cellView":"both","colab":{}},"source":["## Hyper-parameters\n","\n","epochs = 5\n","lr = 2e-5\n","eps = 1e-8\n","num_labels = 2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"97kRYkBCYiGd","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Train\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n","                                                      num_labels=num_labels,\n","                                                      output_attentions = False,\n","                                                      output_hidden_states =False)\n","model.cuda()\n","\n","optimizer = AdamW(model.parameters(),\n","                  lr = lr,\n","                  eps = eps)\n","\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                            num_warmup_steps=0,\n","                                            num_training_steps=len(train_dataloader) * epochs)\n","\n","random.seed(random_seed)\n","np.random.seed(random_seed)\n","torch.manual_seed(random_seed)\n","torch.cuda.manual_seed_all(random_seed)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        # It returns different numbers of parameters depending on what arguments\n","        # arge given and what flags are set. For our useage here, it returns\n","        # the loss (because we provided labels) and the \"logits\"--the model\n","        # outputs prior to activation.\n","        loss, logits = model(b_input_ids, \n","                             token_type_ids=None, \n","                             attention_mask=b_input_mask, \n","                             labels=b_labels)\n","        \n","\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    # F1\n","    tfpn_dict = {'TP':0, 'TN':0, 'FP':0, 'FN':0}\n","\n","    # Evaluate data for one epoch\n","    for batch in val_dataloader:\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            # Get the \"logits\" output by the model. The \"logits\" are the output\n","            # values prior to applying an activation function like the softmax.\n","            (loss, logits) = model(b_input_ids, \n","                                   token_type_ids=None, \n","                                   attention_mask=b_input_mask,\n","                                   labels=b_labels)\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","\n","        TP, TN, FP, FN = tfpn(torch.from_numpy(logits), b_labels)\n","        tfpn_dict['TP'] += TP\n","        tfpn_dict['TN'] += TN\n","        tfpn_dict['FP'] += FP\n","        tfpn_dict['FN'] += FN\n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n","\n","    # calculate f1 score for this val epoch\n","    f1_score = f1(tfpn_dict).item()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(val_dataloader)\n","\n","    print('Validation - Loss: {:.3f} | Accuracy: {:.3f} | BCE: {:.3f} | F1: {:.3f}'.format(avg_val_loss, avg_val_accuracy, avg_BCE_loss, f1_score))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Train Loss': avg_train_loss,\n","            'Val Loss': avg_val_loss,\n","            'Val Accuracy': avg_val_accuracy,\n","            'Val F1': f1_score\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4c_V8yR8CZYG","colab_type":"code","cellView":"form","outputId":"bf7e8f23-be71-402c-bb93-fbb9b1f899d6","executionInfo":{"status":"ok","timestamp":1588407911424,"user_tz":240,"elapsed":24270,"user":{"displayName":"Jie Lyu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQrhIVVXzQU_KkxCjkumcX5SB8g_Uk4VndpwJ4=s64","userId":"10977586217171880674"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#@title Predict\n","model.eval()\n","\n","predictions = []\n","\n","for batch in prediction_dataloader:\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask = batch\n","\n","    # Telling the model not to compute or store gradients, saving memory and \n","    # speeding up prediction\n","    with torch.no_grad():\n","        # Forward pass, calculate logit predictions\n","        outputs = model(b_input_ids, token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","\n","    logits = outputs[0]\n","\n","    # Move logits and labels to CPU\n","    logits = logits.detach().cpu()\n","\n","    # logits to label\n","    softmax = torch.nn.Softmax(dim=1)\n","    sm = softmax(logits)\n","    pred = torch.argmax(sm, dim=1)\n","\n","    # Store predictions and true labels\n","    predictions += pred.tolist()\n","\n","sample_submission = pd.read_csv(\"sample_submission.csv\")\n","sample_submission[\"target\"] = predictions\n","sample_submission.to_csv(\"submission.csv\", index=False)\n","\n","print('submission.csv generated')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["submission.csv generated\n"],"name":"stdout"}]}]}