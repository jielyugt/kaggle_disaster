=================================================================

[TODO]

=================================================================

[Architecture]

<naive>
    sklearn.feature_extraction + sklearn.linear_model
    https://www.kaggle.com/philculliton/nlp-getting-started-tutorial

<word2vec + linear>
    pass the addition or concatenation of pre-tained word2vec vectors to a linear model
    an easy improvement from the naive method
	result:
		RidgeClassifier + addition gives small improvement over naive with cross_val_score of 0.7 and kaggle score of 0.78834
		RidgeClassifier + concatenation gives a bad cross_val_score of 0.6, maybe 9000d vector is too much for RidgeClassifier
		SVC + addition gives cross_val_score of 0.72 and kaggle score of 0.78629 
		SVC + concatenation gives cross_val_score of 0.72 
	conclusion:
		word2vec did not result in significant improvement from one-hot using liear models
		linear models are not capable to harness the high dimensional concatenation of word embeddings

<word2vec + CNN>
    run CNN with filters of difference window sizes on concatenation of pre-trained word2vec vectors
    https://arxiv.org/abs/1408.5882
	result:
		best f1 score around 90 so far (0.01 validation split), Kaggle LB 0.78
	conclusion:
		word2evc has too high a dimensionality, thus a deep model is needed to harness it
		dataset is too small for such a deep model to avoid overfitting

<embedding layer + CNN>
	instead of word2vec, append an embedding layer to CNN, that way we can reduce the dimensionality of word embeddings and reduce the model size?
	result:
		best f1 score around 92 (0.01 validation split), Kaggle score 0.794

<BERT>
    to get better performance in this competition and future competitions
	load pre-trained bert model, then finetune on our task
	GPU OOM might be a problem, try Colab first
	result:
		Kaggle LB 0.800

=================================================================

[Data Cleaning]

remove @
remove non-alphabetic tokens
remove stopwords
remove urls
TODO

=================================================================

[Loading word2vec]

wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz
gunzip GoogleNews-vectors-negative300.bin.gz
import gensim
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)

=================================================================

[Useful Links]

<Getting Started>
	https://towardsdatascience.com/using-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751
	https://www.kaggle.com/philculliton/nlp-getting-started-tutorial

<CNN Examples>
	https://github.com/Shawn1993/cnn-text-classification-pytorch/blob/master/model.py
	https://github.com/keras-team/keras/blob/master/examples/imdb_cnn.py
