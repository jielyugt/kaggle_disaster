[Architecture]

<naive>
    sklearn.feature_extraction + sklearn.linear_model
    https://www.kaggle.com/philculliton/nlp-getting-started-tutorial

<word2vec + linear>
    pass the addition or concatenation of pre-tained word2vec vectors to a linear model
    an easy improvement from the naive method
	result:
		RidgeClassifier + addition gives small improvement over naive with cross_val_score of 0.7 and kaggle score of 0.78834
		RidgeClassifier + concatenation gives a bad cross_val_score of 0.6, maybe 9000d vector is too much for RidgeClassifier
		SVC + addition gives cross_val_score of 0.72 and kaggle score of 0.78629 
		SVC + concatenation gives cross_val_score of 0.72 
	conclusion:
		word2vec did not result in significant improvement from one-hot using liear models
		linear models are not capable to harness the high dimensional concatenation of word embeddings

<word2vec + CNN>
    run CNN with filters of difference window sizes on concatenation of pre-trained word2vec vectors
    https://arxiv.org/abs/1408.5882

<BERT>
    TODO

=================================================================

[Data Cleaning]

remove @
remove non-alphabetic tokens
remove stopwords
remove urls
TODO

=================================================================

[Loading word2vec]

wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz
gunzip GoogleNews-vectors-negative300.bin.gz
import gensim
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)

=================================================================

[Useful Links]

<Getting Started>
	https://towardsdatascience.com/using-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751
	https://www.kaggle.com/philculliton/nlp-getting-started-tutorial

<CNN Examples>
	https://github.com/Shawn1993/cnn-text-classification-pytorch/blob/master/model.py
	https://github.com/keras-team/keras/blob/master/examples/imdb_cnn.py
