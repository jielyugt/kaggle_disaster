{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn import linear_model, model_selection\n",
    "import gensim\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for padding\n",
    "max_len = 30\n",
    "\n",
    "# load pre-trained word2vec model, see notes.txt for how to download the .bin file\n",
    "w2v_model_name = '../GoogleNews-vectors-negative300.bin'\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(w2v_model_name, binary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text_arr):\n",
    "    '''remove non-alphabetic tokens and filter out stopwords\n",
    "    \n",
    "    Args:\n",
    "        text_arr: list of strings, each representing sentences\n",
    "    \n",
    "    Returns:\n",
    "        2d list of strings, each representing words\n",
    "    '''\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "    clean = []\n",
    "    \n",
    "    for sentence in text_arr:\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        list_sentence = []\n",
    "        for word in tokens:\n",
    "            word = word.strip('#').lower()\n",
    "            if word.isalpha() and not word in stop_words:\n",
    "                list_sentence.append(word)\n",
    "        clean.append(list_sentence)\n",
    "    return clean\n",
    "\n",
    "def text_to_vec_addition(text_arr, embedding_model):\n",
    "    '''embed sentences to vectors by adding word vectors together\n",
    "    \n",
    "    Args:\n",
    "        text_arr: 2d list of strings, each representing words\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (number of sentences, word vector_size)\n",
    "    '''\n",
    "    np_array = np.zeros((len(text_arr), embedding_model.vector_size))\n",
    "    for i, sentence in enumerate(text_arr):\n",
    "        sentence_embedding = np.zeros(embedding_model.vector_size)\n",
    "        for word in sentence:\n",
    "            if word in embedding_model.vocab:\n",
    "                sentence_embedding += embedding_model[word]\n",
    "        np_array[i] = sentence_embedding\n",
    "    return np_array\n",
    "\n",
    "def text_to_vec_concatenation(text_arr, embedding_model):\n",
    "    '''embed sentences to vectors by concatenating word vectors together\n",
    "    \n",
    "    Args:\n",
    "        text_arr: 2d list of strings, each representing words\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (number of sentences, word vector_size*max_len)\n",
    "    '''\n",
    "    vector_array = []\n",
    "    for i, sentence in enumerate(text_arr):\n",
    "        sentence_embedding = []\n",
    "        for word in sentence:\n",
    "            if word in embedding_model.vocab:\n",
    "                sentence_embedding += embedding_model[word].tolist()\n",
    "        vector_array.append(sentence_embedding)\n",
    "    np_array = sequence.pad_sequences(vector_array, maxlen=max_len*embedding_model.vector_size, padding='post', truncating='post', dtype='float32')\n",
    "    return np_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text data\n",
    "train_df = pd.read_csv('../train.csv')\n",
    "train_x_str = clean_text([each for each in train_df['text']])\n",
    "train_y = [each for each in train_df['target']]\n",
    "\n",
    "# convert text to word2vec vectors\n",
    "train_x = text_to_vec_addition(train_x_str, w2v_model)\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up classifier\n",
    "clf = linear_model.RidgeClassifier()\n",
    "scores = model_selection.cross_val_score(clf, train_x, train_y, cv=3, scoring='f1')\n",
    "print(scores)\n",
    "\n",
    "# fit classifier\n",
    "clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text data\n",
    "test_df = pd.read_csv('../test.csv')\n",
    "test_x_str = clean_text([each for each in test_df['text']])\n",
    "\n",
    "# convert text to word2vec vectors, using addition\n",
    "test_x = text_to_vec_addition(test_x_str, w2v_model)\n",
    "\n",
    "# get test results\n",
    "sample_submission = pd.read_csv(\"../sample_submission.csv\")\n",
    "sample_submission[\"target\"] = clf.predict(test_x)\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
