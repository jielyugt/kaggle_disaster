{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports (if tensorflow.keras.xxx doesn't work, try just keras.xxx)\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding, Input\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import gensim\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyper-parameters\n",
    "\n",
    "max_features = 2000\n",
    "maxlen = 12\n",
    "batch_size = 128\n",
    "embedding_dims = 300\n",
    "filters = 60\n",
    "kernel_size = 30\n",
    "hidden_dims = 200\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load pre-trained word2vec model, see notes.txt for how to download the .bin file\n",
    "\n",
    "w2v_model_name = '../GoogleNews-vectors-negative300.bin'\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(w2v_model_name, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text_arr):\n",
    "    '''remove non-alphabetic tokens and filter out stopwords\n",
    "    \n",
    "    Args:\n",
    "        text_arr: list of strings, each representing sentences\n",
    "    \n",
    "    Returns:\n",
    "        2d list of strings, each representing words\n",
    "    '''\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "    clean = []\n",
    "    \n",
    "    for sentence in text_arr:\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        list_sentence = []\n",
    "        for word in tokens:\n",
    "            word = word.strip('#').lower()\n",
    "            if word.isalpha() and not word in stop_words:\n",
    "                list_sentence.append(word)\n",
    "        clean.append(list_sentence)\n",
    "    return clean\n",
    "\n",
    "\n",
    "def text_to_vec_concatenation(text_arr, embedding_model, maxlen):\n",
    "    '''embed sentences to vectors by concatenating word vectors together\n",
    "    \n",
    "    Args:\n",
    "        text_arr: 2d list of strings, each representing words\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (number of sentences, word vector_size*max_len)\n",
    "    '''\n",
    "    vector_array = []\n",
    "    for i, sentence in enumerate(text_arr):\n",
    "        sentence_embedding = []\n",
    "        for word in sentence:\n",
    "            if word in embedding_model.vocab:\n",
    "                sentence_embedding += embedding_model[word].tolist()\n",
    "        vector_array.append(sentence_embedding)\n",
    "    np_array = sequence.pad_sequences(vector_array, maxlen=maxlen*embedding_model.vector_size, padding='post', truncating='post', dtype='float32')\n",
    "    return np_array\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "\n",
    "# load and clean text from .csv\n",
    "df_train = pd.read_csv('../train.csv')\n",
    "x_train_str = clean_text([each for each in df_train['text']])\n",
    "y_train = np.asarray([each for each in df_train['target']], dtype='float32')\n",
    "\n",
    "# convert text to word2vec vectors\n",
    "x_train = text_to_vec_concatenation(x_train_str, w2v_model, maxlen)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# model.add(Embedding(max_features,\n",
    "#                     embedding_dims,\n",
    "#                     input_length=maxlen))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# using word2vec\n",
    "model.add(Input(shape=(embedding_dims*maxlen, 1)))\n",
    "\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## compile and train\n",
    "\n",
    "optimizer = Adam(lr=0.0005)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy', f1_m])\n",
    "\n",
    "model.fit(np.expand_dims(x_train, axis=2),\n",
    "          y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "\n",
    "# load text data\n",
    "df_test = pd.read_csv('../test.csv')\n",
    "x_test_str = clean_text([each for each in df_test['text']])\n",
    "\n",
    "# convert text to word2vec vectors\n",
    "x_test = text_to_vec_concatenation(x_test_str, w2v_model, maxlen)\n",
    "\n",
    "# get test results\n",
    "sample_submission = pd.read_csv(\"../sample_submission.csv\")\n",
    "sample_submission[\"target\"] = np.where(model.predict(np.expand_dims(x_test, axis=2)) > 0.5, 1, 0)\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
