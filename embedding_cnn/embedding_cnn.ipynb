{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence, text\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding, Input\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_features = 2000\n",
    "maxlen = 15\n",
    "batch_size = 128\n",
    "embedding_dims = 30\n",
    "filters = 60\n",
    "kernel_size = 3\n",
    "hidden_dims = 200\n",
    "epochs = 30\n",
    "l2_loss_lambda = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text_arr):\n",
    "    '''remove non-alphabetic tokens and filter out stopwords\n",
    "    \n",
    "    Args:\n",
    "        text_arr: list of strings, each representing sentences\n",
    "    \n",
    "    Returns:\n",
    "        2d list of strings, each representing words\n",
    "    '''\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "    clean = []\n",
    "    \n",
    "    for sentence in text_arr:\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        list_sentence = []\n",
    "        for word in tokens:\n",
    "            word = word.strip('#').lower()\n",
    "            if word.isalpha() and not word in stop_words:\n",
    "                list_sentence.append(word)\n",
    "        clean.append(list_sentence)\n",
    "    return clean\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def macro_soft_f1(y, y_hat):\n",
    "\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.cast(y_hat, tf.float32)\n",
    "    tp = tf.reduce_sum(y_hat * y, axis=0)\n",
    "    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n",
    "    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n",
    "    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    cost = 1 - soft_f1 # reduce 1 - soft-f1 in order to increase soft-f1\n",
    "    macro_cost = tf.reduce_mean(cost) # average on all labels\n",
    "    \n",
    "    return macro_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../train.csv')\n",
    "x_train_str = clean_text([each for each in df_train['text']])\n",
    "y_train = np.asarray([each for each in df_train['target']])\n",
    "\n",
    "df_test = pd.read_csv('../test.csv')\n",
    "x_test_str = clean_text([each for each in df_test['text']])\n",
    "\n",
    "t = text.Tokenizer(num_words=max_features)\n",
    "t.fit_on_texts(x_train_str)\n",
    "x_train_seq = t.texts_to_sequences(x_train_str)\n",
    "x_test_seq = t.texts_to_sequences(x_test_str)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train_seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "x_test = sequence.pad_sequences(x_test_seq, maxlen=maxlen, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l2 = None if l2_loss_lambda is None else regularizers.l2(l2_loss_lambda)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 kernel_regularizer=l2,\n",
    "                 strides=1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.0001, amsgrad=True)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy', f1_m])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "\n",
    "sample_submission = pd.read_csv(\"../sample_submission.csv\")\n",
    "sample_submission[\"target\"] = np.where(model.predict(x_test) > 0.5, 1, 0)\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
